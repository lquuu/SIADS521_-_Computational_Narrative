{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28aae5f1-9899-4e87-b85d-8033beb87d81",
   "metadata": {},
   "source": [
    "# Assignment 4: A Computational Narrative with Strava Data\n",
    "# Part 1: Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef41f49-d98f-4f24-95ba-cefb75d950fb",
   "metadata": {},
   "source": [
    "## Rule et al.'s Rules (Discussion in Part 3)\n",
    "1. **Rule 2**: Document the process, not just the results\n",
    "2. **Rule 3**: Use cell divisions to make steps clear\n",
    "3. **Rule 8**: Share and explain your data\n",
    "4. **Rule 9**: Design your notebooks to be read, run, and explored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca7f4b-f76c-45f5-a3a4-b7af3ef1b7e8",
   "metadata": {},
   "source": [
    "1. (20%) Are you making a compelling computational narrative, judged in part by **Rule et al’s ten rules for computational analyses**?\n",
    "   - You don’t need to follow all of the rules all of the time, but you must **explicitly indicate at the header** of each notebook which rules you adhered to and what the evidence was.\n",
    "   - While there are no hard limits on the number of rules you should address, I expect **at least three rules** would be able to be discussed for a notebook of this size, and that discussion and evidence of how you aligned with those rules would be on the order of **1-2 paragraphs per rule**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1c952-765d-45b5-b956-171cafd04dfd",
   "metadata": {},
   "source": [
    "2. (35%) Have you demonstrated that you have a solid grasp of **at least three** of the basic visual analysis techniques\n",
    "in this class **(scatter, box, line, violin, histograms, heatmaps, probability plots, treemaps, sploms)** and\n",
    "that they were appropriate for the analysis/data you were investigating?\n",
    "\n",
    "    You get equal grades for **each plot type (15% each: total 45% for 3?)**, and grades for a given plot will be broken down into three equal categories (5% each): [Strava]\n",
    "    - i. The mechanics of generating a reasonable plot from the data you are working with.\n",
    "    - ii. The justification for the plot and the insight as a result, as described by your computational narrative.\n",
    "    - iii. Making the plot rock visually, by embedding advanced features ranging from the aesthetic (color, form) to the informational (callouts, annotations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e066cf-fd66-4454-a27e-8b35a81a4bfd",
   "metadata": {},
   "source": [
    "3. (15%) Have you demonstrated that you have a solid grasp of at l east one of the more advanced visual analysis techniques in this class?\n",
    "\n",
    "   (Don’t use any visualizations listed as basic plots above. You can explore a new visualization technique which the lecture didn’t teach you, or you can even come up with a combination of multiple types of plots to generate an advanced plot) and that it was appropriate for the analysis/data you were investigating?\n",
    "\n",
    "   The grading rubric is the same as the basic plots. You may use other advanced plots with permission in this category (ask first to ensure they seem reasonably advanced)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc61081e-1f6d-4c91-b79a-c8d823d5d338",
   "metadata": {},
   "source": [
    "4. (20%) Are you able to provide an interesting and defensible analysis?\n",
    "\n",
    "   For the both data, your visualization should help Professor Brooks understand what this data **means**. If your data science discovery will make the client happy then this part of the overall grade tilts up towards 20%. If there are obvious things you should have looked at then it tilts down towards 0%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0cd049-057e-429c-b03d-6e073db6797a",
   "metadata": {},
   "source": [
    "5. (10%) Are the final results displayed in a dashboard that tells your story?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1da72-b608-489e-8c25-0b05c02fb940",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "\n",
    "The data are in a variety of different units. A previous student noted the following units:\n",
    "| Data | Units |\n",
    "| ----------- | ----------- |\n",
    "| Cadence | rpm |\n",
    "| Ground Time | milliseconds |\n",
    "| Vertical Oscillation | centimeters |\n",
    "| Distance, Altitude, & Enhanced Altitude | meters |\n",
    "| Longitude & Latitude | semicircles (radians) |\n",
    "| Air & Form Power | watts |\n",
    "| Leg Spring Stiffness | kN/m |\n",
    "| Speed | m/s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42720d8-6676-42a8-aedb-9a120d010ce6",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6681ae5-8dd1-43e4-a0a8-cf8ad26c6225",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "I will be importing the libraries I expect I will need. I may import more later on in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8590f988-97a2-4662-8201-bfcc20f81ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc865eb6-5833-4b0d-9b12-88195c939d1d",
   "metadata": {},
   "source": [
    "### Uploading Strava Data File\n",
    "\n",
    "The data I used for this assignment was readily available in our course materials on Jupyter. The data was provided to us with permission from Professor Brooks with the understanding that we would honor his privacy and not distribute it without his permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abbe2a2e-12b6-43c7-b983-7a7f2c00f951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Air Power</th>\n",
       "      <th>Cadence</th>\n",
       "      <th>Form Power</th>\n",
       "      <th>Ground Time</th>\n",
       "      <th>Leg Spring Stiffness</th>\n",
       "      <th>Power</th>\n",
       "      <th>Vertical Oscillation</th>\n",
       "      <th>altitude</th>\n",
       "      <th>cadence</th>\n",
       "      <th>datafile</th>\n",
       "      <th>...</th>\n",
       "      <th>enhanced_speed</th>\n",
       "      <th>fractional_cadence</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>position_lat</th>\n",
       "      <th>position_long</th>\n",
       "      <th>speed</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unknown_87</th>\n",
       "      <th>unknown_88</th>\n",
       "      <th>unknown_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>activities/2675855419.fit.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-07-08 21:04:03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>activities/2675855419.fit.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-07-08 21:04:04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.0</td>\n",
       "      <td>activities/2675855419.fit.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>1.316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>2019-07-08 21:04:07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3747.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>activities/2675855419.fit.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>1.866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>504432050.0</td>\n",
       "      <td>-999063637.0</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>2019-07-08 21:04:14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3798.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>activities/2675855419.fit.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>1.894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>504432492.0</td>\n",
       "      <td>-999064534.0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>2019-07-08 21:04:15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Air Power  Cadence  Form Power  Ground Time  Leg Spring Stiffness  Power  \\\n",
       "0        NaN      NaN         NaN          NaN                   NaN    NaN   \n",
       "1        NaN      NaN         NaN          NaN                   NaN    NaN   \n",
       "2        NaN      NaN         NaN          NaN                   NaN    NaN   \n",
       "3        NaN      NaN         NaN          NaN                   NaN    NaN   \n",
       "4        NaN      NaN         NaN          NaN                   NaN    NaN   \n",
       "\n",
       "   Vertical Oscillation  altitude  cadence                      datafile  ...  \\\n",
       "0                   NaN       NaN      0.0  activities/2675855419.fit.gz  ...   \n",
       "1                   NaN       NaN      0.0  activities/2675855419.fit.gz  ...   \n",
       "2                   NaN       NaN     54.0  activities/2675855419.fit.gz  ...   \n",
       "3                   NaN    3747.0     77.0  activities/2675855419.fit.gz  ...   \n",
       "4                   NaN    3798.0     77.0  activities/2675855419.fit.gz  ...   \n",
       "\n",
       "   enhanced_speed  fractional_cadence  heart_rate  position_lat  \\\n",
       "0           0.000                 0.0        68.0           NaN   \n",
       "1           0.000                 0.0        68.0           NaN   \n",
       "2           1.316                 0.0        71.0           NaN   \n",
       "3           1.866                 0.0        77.0   504432050.0   \n",
       "4           1.894                 0.0        80.0   504432492.0   \n",
       "\n",
       "   position_long   speed            timestamp  unknown_87 unknown_88  \\\n",
       "0            NaN     0.0  2019-07-08 21:04:03         0.0      300.0   \n",
       "1            NaN     0.0  2019-07-08 21:04:04         0.0      300.0   \n",
       "2            NaN  1316.0  2019-07-08 21:04:07         0.0      300.0   \n",
       "3   -999063637.0  1866.0  2019-07-08 21:04:14         0.0      100.0   \n",
       "4   -999064534.0  1894.0  2019-07-08 21:04:15         0.0      100.0   \n",
       "\n",
       "   unknown_90  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.realpath(\"/home/jovyan/work/resources/course_assignments/assets/strava.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n",
    "#df.shape (40649, 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89dcce6-97a0-4bf4-af5c-099c72553220",
   "metadata": {},
   "source": [
    "### Gaining an Understanding of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da46b8fa-f68a-4eb6-82af-df7891ba93e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Air Power', 'Cadence', 'Form Power', 'Ground Time',\n",
       "       'Leg Spring Stiffness', 'Power', 'Vertical Oscillation', 'altitude',\n",
       "       'cadence', 'datafile', 'distance', 'enhanced_altitude',\n",
       "       'enhanced_speed', 'fractional_cadence', 'heart_rate', 'position_lat',\n",
       "       'position_long', 'speed', 'timestamp', 'unknown_87', 'unknown_88',\n",
       "       'unknown_90'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c6bf51-b647-4f4d-9c31-5877ae4039c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unknown_88': [300.0, 100.0, nan]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what types of values of in each column\n",
    "# I interchanged the \"column\" parameter to get an idea of each column\n",
    "\n",
    "def get_unique_values(df, column):\n",
    "    unique_values = {}\n",
    "    unique_values[column] = df[column].unique().tolist()\n",
    "    return unique_values\n",
    "\n",
    "get_unique_values(df, 'unknown_88')\n",
    "\n",
    "# The code below calculates how many unique values are in the column\n",
    "# len(get_unique_values(df, 'unknown_88')['unknown_88'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f4e6d87-6436-4a98-84c1-03625fccb5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Air Power               56.107161\n",
       "Cadence                 56.094861\n",
       "Form Power              56.107161\n",
       "Ground Time             56.094861\n",
       "Leg Spring Stiffness    56.107161\n",
       "Power                   56.094861\n",
       "Vertical Oscillation    56.094861\n",
       "altitude                63.332431\n",
       "cadence                  0.054122\n",
       "datafile                 0.000000\n",
       "distance                 0.000000\n",
       "enhanced_altitude        0.125464\n",
       "enhanced_speed           0.024601\n",
       "fractional_cadence       0.054122\n",
       "heart_rate               5.643435\n",
       "position_lat             0.472336\n",
       "position_long            0.472336\n",
       "speed                   63.275849\n",
       "timestamp                0.000000\n",
       "unknown_87               0.054122\n",
       "unknown_88               5.643435\n",
       "unknown_90              54.198135\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "nan_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cdc7c5-22f5-4ee2-8498-7432575faf84",
   "metadata": {},
   "source": [
    "There are a LOT of `NaN`s! In fact, some columns (e.g., `'Air Power'`, `'speed'`) have more `NaN` values than actual values. Because of the huge variance across the data types, I think that the best way to deal with this would be to create charts that take this into account. Thus, instead of dropping all NaNs (which would clear out a substantial amount of data), I'd like to keep as much of it as possible, and only drop the necessary pieces of data per chart.\n",
    "\n",
    "This means that I will be creating each chart and dropping or replacing values **dynamically** based on what I believe best suits the needs of the visualization and also best represents the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79b5be1-9c96-4d4e-91ac-614c28457425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Power               float64\n",
      "Cadence                 float64\n",
      "Form Power              float64\n",
      "Ground Time             float64\n",
      "Leg Spring Stiffness    float64\n",
      "Power                   float64\n",
      "Vertical Oscillation    float64\n",
      "altitude                float64\n",
      "cadence                 float64\n",
      "datafile                 object\n",
      "distance                float64\n",
      "enhanced_altitude       float64\n",
      "enhanced_speed          float64\n",
      "fractional_cadence      float64\n",
      "heart_rate              float64\n",
      "position_lat            float64\n",
      "position_long           float64\n",
      "speed                   float64\n",
      "timestamp                object\n",
      "unknown_87              float64\n",
      "unknown_88              float64\n",
      "unknown_90              float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d986058-acc5-46ec-b354-6137baa8c41c",
   "metadata": {},
   "source": [
    "### Type Conversions\n",
    "As shown above, `timestamp` is currently an `object` dtype. I will like to convert the `timestamp` entries to the correct datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631550a9-6031-494a-a181-d9abf2ec3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "print(df['timestamp'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "213b6ceb-57a8-4467-8368-1bfd8bc3961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 11, 12, 14, 15, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "df['hour'] = df['timestamp'].dt.hour\n",
    "unique_hours = df['hour'].unique()\n",
    "print(sorted(unique_hours))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed4089-554e-4bcc-b116-d0a71fde2f4d",
   "metadata": {},
   "source": [
    "### Considering the Correct Time Zone\n",
    "\n",
    "Looking at the unique hours above, it appears that the professor's workouts typically take place:\n",
    "- Around noon\n",
    "- In the evenings\n",
    "- Around midnight\n",
    "- Sometimes 2-3 in the morning?!\n",
    "\n",
    "While this might be true, let's consider if an alternate theory would give us more reasonable data. In other words, could the fitness tracker be recording the professor's workouts using UTC times, even though the professor might not live in a UTC+00:00 time zone?\n",
    "\n",
    "Let's dive in.\n",
    "\n",
    "A quick Google search turns up the professor's office on the University of Michigan's Ann Arbor campus at the School of Information [https://www.si.umich.edu/people/christopher-brooks]. Thus, let us work under the assumption that it is highly likely that Professor Brooks lives in the Ann Arbor area (or, at the very least, in the Eastern Time Zone). This means that the times should be converted into **Eastern Time**.\n",
    "\n",
    "EDT is the time zone used in summer and spring, while EST is the time zone used in winter and autumn.\n",
    "- In 2019, \"Spring Forward\" occurred on March 10, 2019\n",
    "- \"Fall Back\" occurred on November 3, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e3dd13-c22c-41da-b50d-6a0901a83e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "df['month'] = df['timestamp'].dt.month\n",
    "unique_months = df['month'].unique()\n",
    "print(sorted(unique_months))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1ce8a-2e0e-4ce9-9156-58341f0c21a5",
   "metadata": {},
   "source": [
    "Luckily for us, the data takes place across July through October 2019, so we do not need to take Daylight Savings into account.\n",
    "\n",
    "So let's continue trying to convert our data from UTC to EDT. A quick Google search turns up that EDT is **UTC-04:00**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ff0da1-e83b-4f0e-97f7-951f2dbb5af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pytz.lazy.LazyList.__new__.<locals>.LazyList'>\n",
      "America/Adak\n",
      "America/Anchorage\n",
      "America/Anguilla\n",
      "America/Antigua\n",
      "America/Araguaina\n",
      "America/Argentina/Buenos_Aires\n",
      "America/Argentina/Catamarca\n",
      "America/Argentina/ComodRivadavia\n",
      "America/Argentina/Cordoba\n",
      "America/Argentina/Jujuy\n",
      "America/Argentina/La_Rioja\n",
      "America/Argentina/Mendoza\n",
      "America/Argentina/Rio_Gallegos\n",
      "America/Argentina/Salta\n",
      "America/Argentina/San_Juan\n",
      "America/Argentina/San_Luis\n",
      "America/Argentina/Tucuman\n",
      "America/Argentina/Ushuaia\n",
      "America/Aruba\n",
      "America/Asuncion\n",
      "America/Atikokan\n",
      "America/Atka\n",
      "America/Bahia\n",
      "America/Bahia_Banderas\n",
      "America/Barbados\n",
      "America/Belem\n",
      "America/Belize\n",
      "America/Blanc-Sablon\n",
      "America/Boa_Vista\n",
      "America/Bogota\n",
      "America/Boise\n",
      "America/Buenos_Aires\n",
      "America/Cambridge_Bay\n",
      "America/Campo_Grande\n",
      "America/Cancun\n",
      "America/Caracas\n",
      "America/Catamarca\n",
      "America/Cayenne\n",
      "America/Cayman\n",
      "America/Chicago\n",
      "America/Chihuahua\n",
      "America/Ciudad_Juarez\n",
      "America/Coral_Harbour\n",
      "America/Cordoba\n",
      "America/Costa_Rica\n",
      "America/Creston\n",
      "America/Cuiaba\n",
      "America/Curacao\n",
      "America/Danmarkshavn\n",
      "America/Dawson\n",
      "America/Dawson_Creek\n",
      "America/Denver\n",
      "America/Detroit\n",
      "America/Dominica\n",
      "America/Edmonton\n",
      "America/Eirunepe\n",
      "America/El_Salvador\n",
      "America/Ensenada\n",
      "America/Fort_Nelson\n",
      "America/Fort_Wayne\n",
      "America/Fortaleza\n",
      "America/Glace_Bay\n",
      "America/Godthab\n",
      "America/Goose_Bay\n",
      "America/Grand_Turk\n",
      "America/Grenada\n",
      "America/Guadeloupe\n",
      "America/Guatemala\n",
      "America/Guayaquil\n",
      "America/Guyana\n",
      "America/Halifax\n",
      "America/Havana\n",
      "America/Hermosillo\n",
      "America/Indiana/Indianapolis\n",
      "America/Indiana/Knox\n",
      "America/Indiana/Marengo\n",
      "America/Indiana/Petersburg\n",
      "America/Indiana/Tell_City\n",
      "America/Indiana/Vevay\n",
      "America/Indiana/Vincennes\n",
      "America/Indiana/Winamac\n",
      "America/Indianapolis\n",
      "America/Inuvik\n",
      "America/Iqaluit\n",
      "America/Jamaica\n",
      "America/Jujuy\n",
      "America/Juneau\n",
      "America/Kentucky/Louisville\n",
      "America/Kentucky/Monticello\n",
      "America/Knox_IN\n",
      "America/Kralendijk\n",
      "America/La_Paz\n",
      "America/Lima\n",
      "America/Los_Angeles\n",
      "America/Louisville\n",
      "America/Lower_Princes\n",
      "America/Maceio\n",
      "America/Managua\n",
      "America/Manaus\n",
      "America/Marigot\n",
      "America/Martinique\n",
      "America/Matamoros\n",
      "America/Mazatlan\n",
      "America/Mendoza\n",
      "America/Menominee\n",
      "America/Merida\n",
      "America/Metlakatla\n",
      "America/Mexico_City\n",
      "America/Miquelon\n",
      "America/Moncton\n",
      "America/Monterrey\n",
      "America/Montevideo\n",
      "America/Montreal\n",
      "America/Montserrat\n",
      "America/Nassau\n",
      "America/New_York\n",
      "America/Nipigon\n",
      "America/Nome\n",
      "America/Noronha\n",
      "America/North_Dakota/Beulah\n",
      "America/North_Dakota/Center\n",
      "America/North_Dakota/New_Salem\n",
      "America/Nuuk\n",
      "America/Ojinaga\n",
      "America/Panama\n",
      "America/Pangnirtung\n",
      "America/Paramaribo\n",
      "America/Phoenix\n",
      "America/Port-au-Prince\n",
      "America/Port_of_Spain\n",
      "America/Porto_Acre\n",
      "America/Porto_Velho\n",
      "America/Puerto_Rico\n",
      "America/Punta_Arenas\n",
      "America/Rainy_River\n",
      "America/Rankin_Inlet\n",
      "America/Recife\n",
      "America/Regina\n",
      "America/Resolute\n",
      "America/Rio_Branco\n",
      "America/Rosario\n",
      "America/Santa_Isabel\n",
      "America/Santarem\n",
      "America/Santiago\n",
      "America/Santo_Domingo\n",
      "America/Sao_Paulo\n",
      "America/Scoresbysund\n",
      "America/Shiprock\n",
      "America/Sitka\n",
      "America/St_Barthelemy\n",
      "America/St_Johns\n",
      "America/St_Kitts\n",
      "America/St_Lucia\n",
      "America/St_Thomas\n",
      "America/St_Vincent\n",
      "America/Swift_Current\n",
      "America/Tegucigalpa\n",
      "America/Thule\n",
      "America/Thunder_Bay\n",
      "America/Tijuana\n",
      "America/Toronto\n",
      "America/Tortola\n",
      "America/Vancouver\n",
      "America/Virgin\n",
      "America/Whitehorse\n",
      "America/Winnipeg\n",
      "America/Yakutat\n",
      "America/Yellowknife\n"
     ]
    }
   ],
   "source": [
    "import pytz         # Time Zone Package: https://pypi.org/project/pytz/\n",
    "print(type(pytz.all_timezones))  # Used pytz.all_timezones; but it was a very large list\n",
    "                                 # Noticed that time zones were categorized by continents\n",
    "                                 # I used this to find the an Eastern Time Zone with a city close to Ann Arbor\n",
    "\n",
    "for timezone in pytz.all_timezones:\n",
    "    if \"America\" in timezone:\n",
    "        print(timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f25cd9cc-ea2e-472d-8a90-a05071c35cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2019-07-08 17:04:03-04:00\n",
       "1       2019-07-08 17:04:04-04:00\n",
       "2       2019-07-08 17:04:07-04:00\n",
       "3       2019-07-08 17:04:14-04:00\n",
       "4       2019-07-08 17:04:15-04:00\n",
       "                   ...           \n",
       "40644   2019-10-03 19:04:54-04:00\n",
       "40645   2019-10-03 19:04:56-04:00\n",
       "40646   2019-10-03 19:04:57-04:00\n",
       "40647   2019-10-03 19:05:02-04:00\n",
       "40648   2019-10-03 19:05:05-04:00\n",
       "Name: timestamp, Length: 40649, dtype: datetime64[ns, America/Detroit]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm Times are in UTC\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "\n",
    "# Convert to EDT (pytz takes into account Daylight Savings automatically)\n",
    "# 'America/Detroit' was the closest one to Ann Arbor\n",
    "# Converting using pytz & Pandas: https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.tz_convert.html\n",
    "df['timestamp'] = df['timestamp'].dt.tz_convert('America/Detroit')\n",
    "\n",
    "df['timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a2f63-5b8a-4dfe-a8ca-cdceb2512680",
   "metadata": {},
   "source": [
    "Now let's repeat the process and see the unique hours in which the professor was working out, in EDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685a8644-0bf6-4407-ba43-db1a87573c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "df['hour'] = df['timestamp'].dt.hour\n",
    "unique_hours = df['hour'].unique()\n",
    "print(sorted(unique_hours))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966cd3d-a2d9-4db8-9654-4d7835b69a2a",
   "metadata": {},
   "source": [
    "Ah, these numbers make a lot more sense! The professor is now working out in the mornings (7-11 AM), afternoons (1-4 PM), and evenings (5-11 PM). These times span more typical times a person is awake throughout the day. Thus, I would like to proceed with the EDT times we've converted from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30109e-753e-4254-b98b-ba63c7cdf52c",
   "metadata": {},
   "source": [
    "### Separating Data into Workouts\n",
    "\n",
    "I wanted to consider how many workouts were recorded in total. After manually looking through the data, I thought about the names of datafiles. If Professor Brooks were to manually press 'Start' and 'Stop' on his fitness tracker each time he began and ended his workouts, the workout would be saved as a new file. I checked this below this below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d40baf-90d9-42ac-a86c-68f0756f6d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_unique_values(df, 'datafile')['datafile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d859a-c7d9-4060-ab6b-661ea826f56f",
   "metadata": {},
   "source": [
    "Thus I am led to believe that there are 64 separate files/workouts in the Strava CSV file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
